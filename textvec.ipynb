{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b9364f",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c742e",
   "metadata": {},
   "source": [
    "\n",
    "Machine Learning (ML) models are used for making predictions. Predictions could be about the weather, whether a user clicks on an ad/movie/song, the answer to a question etc. In order to make a prediction the model needs to be provided some input data that contains information that can be used to make a prediction.\n",
    "\n",
    "The way input data is presented to a model is quite critical and can determine how easy it is for a model to extract information from it. LLMs are no different, today we'll dive into how we need to present input data to them.\n",
    "\n",
    "Text Vectorization: Converting text to numbers\n",
    "On receiving input, ML models perform a series of operations like multiplications to provide a numerical output that is translated into a prediction. In the world of LLMs the model is provided with a prompt made up of text, however running the mathematical operations associated with the internal workings of an LLM requires converting the text to numerical values.\n",
    "\n",
    "The conversion of text into numerical values is called text vectorization. A vector is a sequence of numbers and is analogous to an array of numbers in the context of programming. When dealing with ML libraries it’s common for arrays of numbers to be converted to vector objects since they make mathematical operations run more efficiently. For example in numpy to make an array a vector you'd do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba6ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Array of numbers\n",
    "a = [1,2,3]\n",
    "\n",
    "# Convert array to a vector\n",
    "vector_a = np.asarray(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22816e3f",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bd293",
   "metadata": {},
   "source": [
    "\n",
    "Tokenization is the process of breaking a piece of text into its units called tokens. Tokens can be alphabets, words, or groups of alphabets that make up words often called subwords based on the methodology. A tokenizer is an algorithm that's responsible for tokenizing text.\n",
    "\n",
    "The simplest tokenizer that one can imagine (for English) is to split a document at every space character or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cbca2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'sentence', 'It', 'contains', 'punctuation']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_document(document):\n",
    "    # Define a regular expression pattern to \n",
    "    # match spaces and punctuation as separate tokens\n",
    "    pattern = r\"[\\s.,;!?()]+|[.,;!?()]\"\n",
    "\n",
    "    # Use re.split to tokenize the document\n",
    "    tokens = re.split(pattern, document)\n",
    "\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "text = 'sample sentence. It contains punctuation!'\n",
    "\n",
    "tokenize_document(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40713775",
   "metadata": {},
   "source": [
    "# Building a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb08a6",
   "metadata": {},
   "source": [
    "\n",
    "A vocabulary is the set of all tokens that an ML model would be able to recognize. The English language has around 170k words. Imagine how huge this number would be for a multilingual use case.\n",
    "\n",
    "We need to put a cap on the size of our vocabulary to ensure computational efficiency. The size of the vocabulary is often capped by counting the frequency of tokens in a huge corpus of text and choosing the top-k tokens. Where k would correspond to the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3873c52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample', 'sentence', 'some', 'with', 'words'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_top_k_vocab(corpus, k):\n",
    "    # Initialize a Counter to count token frequencies\n",
    "    token_counter = Counter()\n",
    "\n",
    "    # Tokenize and count tokens in each document\n",
    "    for document in corpus:\n",
    "        tokens = tokenize_document(document)\n",
    "        token_counter.update(tokens)\n",
    "\n",
    "    # Get the top 10 tokens by frequency\n",
    "    top_k_tokens = [token for token, _ in \n",
    "                   token_counter.most_common(k)]\n",
    "\n",
    "    return set(top_k_tokens)\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "    \"This is a sample sentence with some words.\",\n",
    "    \"Another sample sentence with some repeating words.\",\n",
    "    \"And yet another sentence to build the vocabulary.\",\n",
    "]\n",
    "\n",
    "build_top_k_vocab(corpus, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3551c",
   "metadata": {},
   "source": [
    "# Converting Tokens to Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734be1c2",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have a vocabulary we assign an id to each token in our vocabulary. This can be done through simple enumeration and maintaining a map/dictionary between the ids and corresponding token. The id map will help us keep track of tokens that are present in a document.\n",
    "\n",
    "The identified set of tokens now have to be translated to features that'll help an ML model extract information. While a token can be represented as a scalar or vector a document is always going to be represented as a vector of the token’s representation that it is made up of.\n",
    "\n",
    "Some of the common ways to encode a document into features are:\n",
    "\n",
    "Binary Document-Term Vector: Each document is represented as a vector whose size is equal to the size of the vocabulary. The id of each token in the vocabulary corresponds to the index position in the vector. A value of 1 is assigned to the index position in the vector, when the corresponding token is present in the document and 0 if it’s absent.\n",
    "\n",
    "Bag of Words (BoW): Similar to approach 1, but the vector’s indices map to the frequency of the token in the document.\n",
    "\n",
    "N-gram vectors: This approach extends the above approach by allowing us to have index positions corresponding to bi-grams, tri-grams etc.\n",
    "\n",
    "Tf-idf: Similar to BoW, but instead of just the frequency a token is assigned a value based on its frequency in a document and how many unique documents in the corpus the token occurs in. The intuition is that tokens that are rare in general but are present a large number of times in a specific document are important, but the tokens that are plentiful across all documents (like the words \"and, an, the, it etc.\") are not.\n",
    "\n",
    "Embeddings: This approach is used in most deep neural networks such as LLMs. Each id is mapped to a unique n-dimensional vector called an embedding. The advantage of this approach is that rather than having a single hand-crafted feature such as the ones mentioned above for each token, an LLM can learn a better high dimensional feature via back propagation. An embedding is supposed to be able to capture the meaning or context around which a token occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcaf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
